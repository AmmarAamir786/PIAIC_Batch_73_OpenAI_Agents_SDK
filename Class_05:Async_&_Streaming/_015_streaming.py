"""
This example demonstrates how to stream an agent's result using agents_sdk.
Instead of waiting for the entire response, you can process and display output tokens as they arrive in real time.

- Runner.run_streamed returns a stream of events, allowing you to handle partial outputs (tokens) as they are generated.
- This is useful for applications where immediate feedback or progressive display is important (e.g., chat UIs, live dashboards).

Comparison to Standard Execution:
- Standard (sync/async) execution waits for the full response before returning anything.
- Streaming allows you to process and display output incrementally, improving perceived responsiveness.
"""

import asyncio
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel

# Set up your Gemini API key here 
gemini_api_key = ""

# 1. Set up the provider to use the Gemini API Key
provider = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Set up the model to use the provider
model = OpenAIChatCompletionsModel(
    model='gemini-2.0-flash',
    openai_client=provider,
)

# 3. Set up the agent to use the model
agent = Agent(
    name="agent",
    instructions="You are a helpful assistant",
    model=model,
)

# 4. Set up the runner to use the agent and generate a joke
async def main(): # step 1: create async main function
    result = Runner.run_streamed( # step 2: use Runner.run_streamed for streaming
        agent,
        input="write an essay on programming in 500 words",
    )

    # Print the agent's final output
    # print(result.final_output)
    
    # Iterate over the streamed events as they arrive from the agent
    async for event in result.stream_events(): # step 3: iterate over stream events asynchronously
        # Check if the event is a raw response event and contains a new token (delta)
        if event.type == "raw_response_event" and hasattr(event.data, 'delta'):
            token = event.data.delta  # Extract the new token generated by the model
            print(token)  # Print each token as soon as it is received (real-time streaming)
    
    
if __name__ == "__main__":
    asyncio.run(main())

# Scenario-based questions:
# 1. What are the advantages of streaming agent results compared to waiting for the full output?
# 2. In what scenarios would streaming be essential (e.g., user experience, long responses)?
# 3. How would you handle errors or interruptions during streaming?
# 4. Why are we not using simple result.final_output?